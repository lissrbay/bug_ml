{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "for summary in tf.compat.v1.train.summary_iterator(\"./lightning_logs_private/scaffle/version_1/events.out.tfevents.1687079795.ip-10-0-0-10.eu-west-1.compute.internal.16796.82\"):\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_fine_TopkAccuracy_1_mean: 0.509\n",
      "bert_fine_TopkAccuracy_1_std: 0.016\n",
      "bert_fine_TopkAccuracy_1_quantile: ( 0.441 , 0.569 )\n",
      "bert_fine_TopkAccuracy_3_mean: 0.695\n",
      "bert_fine_TopkAccuracy_3_std: 0.015\n",
      "bert_fine_TopkAccuracy_3_quantile: ( 0.638 , 0.747 )\n",
      "bert_fine_TopkAccuracy_5_mean: 0.803\n",
      "bert_fine_TopkAccuracy_5_std: 0.013\n",
      "bert_fine_TopkAccuracy_5_quantile: ( 0.757 , 0.852 )\n",
      "bert_our_TopkAccuracy_1_mean: 0.519\n",
      "bert_our_TopkAccuracy_1_std: 0.016\n",
      "bert_our_TopkAccuracy_1_quantile: ( 0.458 , 0.577 )\n",
      "bert_our_TopkAccuracy_3_mean: 0.731\n",
      "bert_our_TopkAccuracy_3_std: 0.014\n",
      "bert_our_TopkAccuracy_3_quantile: ( 0.676 , 0.784 )\n",
      "bert_our_TopkAccuracy_5_mean: 0.819\n",
      "bert_our_TopkAccuracy_5_std: 0.012\n",
      "bert_our_TopkAccuracy_5_quantile: ( 0.775 , 0.862 )\n",
      "bert_caching_TopkAccuracy_1_mean: 0.48\n",
      "bert_caching_TopkAccuracy_1_std: 0.016\n",
      "bert_caching_TopkAccuracy_1_quantile: ( 0.423 , 0.533 )\n",
      "bert_caching_TopkAccuracy_3_mean: 0.678\n",
      "bert_caching_TopkAccuracy_3_std: 0.015\n",
      "bert_caching_TopkAccuracy_3_quantile: ( 0.623 , 0.731 )\n",
      "bert_caching_TopkAccuracy_5_mean: 0.792\n",
      "bert_caching_TopkAccuracy_5_std: 0.013\n",
      "bert_caching_TopkAccuracy_5_quantile: ( 0.736 , 0.834 )\n",
      "scaffle_bert_TopkAccuracy_1_mean: 0.44\n",
      "scaffle_bert_TopkAccuracy_1_std: 0.016\n",
      "scaffle_bert_TopkAccuracy_1_quantile: ( 0.384 , 0.497 )\n",
      "scaffle_bert_TopkAccuracy_3_mean: 0.629\n",
      "scaffle_bert_TopkAccuracy_3_std: 0.016\n",
      "scaffle_bert_TopkAccuracy_3_quantile: ( 0.557 , 0.682 )\n",
      "scaffle_bert_TopkAccuracy_5_mean: 0.722\n",
      "scaffle_bert_TopkAccuracy_5_std: 0.014\n",
      "scaffle_bert_TopkAccuracy_5_quantile: ( 0.663 , 0.77 )\n",
      "scaffle_only_TopkAccuracy_1_mean: 0.417\n",
      "scaffle_only_TopkAccuracy_1_std: 0.016\n",
      "scaffle_only_TopkAccuracy_1_quantile: ( 0.349 , 0.473 )\n",
      "scaffle_only_TopkAccuracy_3_mean: 0.63\n",
      "scaffle_only_TopkAccuracy_3_std: 0.015\n",
      "scaffle_only_TopkAccuracy_3_quantile: ( 0.567 , 0.683 )\n",
      "scaffle_only_TopkAccuracy_5_mean: 0.71\n",
      "scaffle_only_TopkAccuracy_5_std: 0.015\n",
      "scaffle_only_TopkAccuracy_5_quantile: ( 0.654 , 0.761 )\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torchmetrics import Metric\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, Optional, Union\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "from torchmetrics.metric import Metric\n",
    "from torchmetrics.utilities import apply_to_collection\n",
    "\n",
    "class TopkAccuracy(Metric):\n",
    "    def __init__(self, k, dist_sync_on_step=False):\n",
    "        # call `self.add_state`for every internal state that is needed for the metrics computations\n",
    "        # dist_reduce_fx indicates the function that should be used to reduce\n",
    "        # state from multiple processes\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "\n",
    "        self.add_state(\"tp_acc\", default=torch.tensor(0).float(), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total_acc\", default=torch.tensor(0).float(), dist_reduce_fx=\"sum\")\n",
    "        self.k = k\n",
    "        self.name = f\"TopkAccuracy_{k}\"\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor, mask: torch.Tensor, scores: torch.Tensor):\n",
    "        preds = preds * mask.float()\n",
    "        scores = scores * mask.float().unsqueeze(-1)\n",
    "        target = target * mask\n",
    "        # scores = scores * preds.unsqueeze(-1)\n",
    "        #print(target, scores, mask)\n",
    "        scores = scores[:, :, 1]\n",
    "\n",
    "        inds = scores.topk(self.k, dim=0).indices\n",
    "        #print( inds.shape)\n",
    "        gathered = torch.gather(target, 0, inds).sum(0) > 0\n",
    "\n",
    "        # if torch.sum(gathered) > 2:\n",
    "        #     print(scores)\n",
    "        #     print(target)\n",
    "        #     print(gathered)\n",
    "        #     print(inds)\n",
    "\n",
    "        self.tp_acc += torch.sum(gathered)\n",
    "        self.total_acc += torch.sum(torch.any(target, dim=0))\n",
    "\n",
    "    def compute(self):\n",
    "        return (self.tp_acc.float() / max(self.total_acc, 1))\n",
    "\n",
    "def bootstrap( preds, target, masks, scores):\n",
    "        bootstraped_metrics = []\n",
    "        metrics = [TopkAccuracy(1), TopkAccuracy(3), TopkAccuracy(5)]\n",
    "        for i in range(10000):\n",
    "            bootstraped_ids = np.random.choice(preds.shape[1], preds.shape[1], replace=True)\n",
    "            bd_preds, bd_targets, bd_masks, bd_scores = (preds[:, bootstraped_ids], target[:, bootstraped_ids], \n",
    "            masks[:, bootstraped_ids], scores[:, bootstraped_ids])\n",
    "            #print(bd_preds)\n",
    "            #print(bd_scores)\n",
    "            for m in metrics:\n",
    "                m.update(bd_preds, bd_targets, bd_masks, scores=bd_scores)\n",
    "            computed_vals = torch.stack([m.compute() for m in metrics], dim=0)\n",
    "            for m in metrics:\n",
    "                m.reset()\n",
    "            bootstraped_metrics.extend(torch.unsqueeze(computed_vals, dim=0))\n",
    "        return torch.stack(bootstraped_metrics, dim=0).T\n",
    "model_names = [\n",
    "    'bert_fine',\n",
    "    'bert_our',\n",
    "    'bert_caching',\n",
    "    #'deep_analyze',\n",
    "    'scaffle_bert', \n",
    "    'scaffle_only'\n",
    "]\n",
    "quantile = 0.0001\n",
    "\n",
    "model_preds = {\n",
    "}\n",
    "\n",
    "def pretty_print(output_dict):\n",
    "    for k, v in output_dict.items():\n",
    "        if 'quantile' in k:\n",
    "            print(f\"{k}: (\", round(v[0].item(), 3), ',', round(v[1].item(), 3), ')')\n",
    "        else:\n",
    "            print(f\"{k}:\", round(v.item(), 3))\n",
    "\n",
    "\n",
    "for i, logs_save_path in enumerate(sorted(Path(\"/home/centos/bug_ml/new/training/l_logs_private/\").glob(\"*/*\"))):\n",
    "    preds_vals = torch.load(os.path.join(logs_save_path,  'preds.pt'), map_location=torch.device('cpu'))\n",
    "    targets_vals = torch.load(os.path.join(logs_save_path,  'targets_vals.pt'), map_location=torch.device('cpu'))\n",
    "    masks_vals = torch.load(os.path.join(logs_save_path,  'masks_vals.pt'), map_location=torch.device('cpu'))\n",
    "    scores_vals = torch.load(os.path.join(logs_save_path,  'scores_vals.pt'), map_location=torch.device('cpu'))\n",
    "    model_name = model_names[i]\n",
    "    computed_vals = (bootstrap(preds_vals, targets_vals, masks_vals, scores_vals))\n",
    "    model_preds[model_name] = computed_vals.clone()\n",
    "    for i, m in enumerate([TopkAccuracy(1), TopkAccuracy(3), TopkAccuracy(5)]):\n",
    "        output_dict = {}\n",
    "        output_dict[model_name + f\"_{m.name}_mean\"] = computed_vals[i, :].mean(dim=0)\n",
    "        output_dict[model_name + f\"_{m.name}_std\"] = computed_vals[i, :].std(dim=0)\n",
    "        low_q = round(quantile/2, 3)\n",
    "        high_q = round(1-quantile/2, 3)\n",
    "        output_dict[model_name + f\"_{m.name}_quantile\"] = (torch.quantile(computed_vals[i, :], quantile/2, interpolation='lower'),  torch.quantile(computed_vals[i, :], 1-quantile/2, interpolation='lower'))\n",
    "        pretty_print(output_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: TopkAccuracy_1, T-test between bert_our and bert_fine: Ttest_indResult(statistic=45.30423719681878, pvalue=0.0)\n",
      "Metric: TopkAccuracy_1, T-test between bert_caching and bert_fine: Ttest_indResult(statistic=-130.4606141005818, pvalue=0.0)\n",
      "Metric: TopkAccuracy_1, T-test between bert_caching and bert_our: Ttest_indResult(statistic=-175.67499247956457, pvalue=0.0)\n",
      "Metric: TopkAccuracy_1, T-test between scaffle and bert_our: Ttest_indResult(statistic=-457.0323261501074, pvalue=0.0)\n",
      "Metric: TopkAccuracy_1, T-test between scaffle and scaffle_bert: Ttest_indResult(statistic=-104.31055527897986, pvalue=0.0)\n",
      "Metric: TopkAccuracy_3, T-test between bert_our and bert_fine: Ttest_indResult(statistic=175.46935148277979, pvalue=0.0)\n",
      "Metric: TopkAccuracy_3, T-test between bert_caching and bert_fine: Ttest_indResult(statistic=-82.39267408542895, pvalue=0.0)\n",
      "Metric: TopkAccuracy_3, T-test between bert_caching and bert_our: Ttest_indResult(statistic=-256.88911279102376, pvalue=0.0)\n",
      "Metric: TopkAccuracy_3, T-test between scaffle and bert_our: Ttest_indResult(statistic=-484.4411287468266, pvalue=0.0)\n",
      "Metric: TopkAccuracy_3, T-test between scaffle and scaffle_bert: Ttest_indResult(statistic=5.2265004718007075, pvalue=1.7448323354422326e-07)\n",
      "Metric: TopkAccuracy_5, T-test between bert_our and bert_fine: Ttest_indResult(statistic=86.69287154260924, pvalue=0.0)\n",
      "Metric: TopkAccuracy_5, T-test between bert_caching and bert_fine: Ttest_indResult(statistic=-61.61705463576975, pvalue=0.0)\n",
      "Metric: TopkAccuracy_5, T-test between bert_caching and bert_our: Ttest_indResult(statistic=-148.23185462073553, pvalue=0.0)\n",
      "Metric: TopkAccuracy_5, T-test between scaffle and bert_our: Ttest_indResult(statistic=-568.5536271683369, pvalue=0.0)\n",
      "Metric: TopkAccuracy_5, T-test between scaffle and scaffle_bert: Ttest_indResult(statistic=-60.303444315827555, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as ss\n",
    "\n",
    "for i, m in enumerate([TopkAccuracy(1), TopkAccuracy(3), TopkAccuracy(5)]):\n",
    "    output_dict = {}\n",
    "    print(f\"Metric: {m.name}, T-test between bert_our and bert_fine:\", ss.ttest_ind(a=model_preds['bert_our'][i, :], b=model_preds['bert_fine'][i, :], equal_var=True))\n",
    "    print(f\"Metric: {m.name}, T-test between bert_caching and bert_fine:\", ss.ttest_ind(a=model_preds['bert_caching'][i, :], b=model_preds['bert_fine'][i, :], equal_var=True))\n",
    "    print(f\"Metric: {m.name}, T-test between bert_caching and bert_our:\", ss.ttest_ind(a=model_preds['bert_caching'][i, :], b=model_preds['bert_our'][i, :], equal_var=True))\n",
    "    print(f\"Metric: {m.name}, T-test between scaffle and bert_our:\", ss.ttest_ind(a=model_preds['scaffle_only'][i, :], b=model_preds['bert_our'][i, :]))\n",
    "    print(f\"Metric: {m.name}, T-test between scaffle and scaffle_bert:\", ss.ttest_ind(a=model_preds['scaffle_only'][i, :], b=model_preds['scaffle_bert'][i, :]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/centos/bug_ml/new/training/l_logs_private/bert/version_0\n",
      "/home/centos/bug_ml/new/training/l_logs_private/bert/version_1\n",
      "/home/centos/bug_ml/new/training/l_logs_private/bert_caching/version_0\n",
      "/home/centos/bug_ml/new/training/l_logs_private/deep_analyze/version_1\n",
      "/home/centos/bug_ml/new/training/l_logs_private/scaffle/scaffle_bert\n",
      "/home/centos/bug_ml/new/training/l_logs_private/scaffle/scaffle_only\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    'bert',\n",
    "    'bert',\n",
    "    'bert_caching',\n",
    "    'deep_analyze',\n",
    "    'scaffle_bert', \n",
    "    'scaffle_only'\n",
    "]\n",
    "for logs_save_path in sorted(Path(\"/home/centos/bug_ml/new/training/l_logs_private/\").glob(\"*/*\")):\n",
    "    print(logs_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_diff_predictions(logdir, our_best_model=''):\n",
    "    models_preds = {}\n",
    "    for dir in glob.glob(logdir + \"/*\"):\n",
    "        model_name = dir.split(\"/\")[-1]\n",
    "\n",
    "        models_preds[model_name] = torch.load(model_name + '_preds')\n",
    "\n",
    "    our_best_model_preds = models_preds[our_best_model]\n",
    "    split_on_batches = torch.load('preds_lens')\n",
    "    split_on_batches = torch.cumsum(split_on_batches, dim=0)\n",
    "    our_best_model_preds_batched = torch.tensor_split(split_on_batches, dim=0)\n",
    "    for model_name, model_preds in models_preds.items():\n",
    "        if model_name != our_best_model:\n",
    "            model_preds_batched = torch.tensor_split(split_on_batches, dim=0)\n",
    "            i = 0\n",
    "            for our_best_model_pred, model_pred in zip(our_best_model_preds, model_preds):\n",
    "                our_scores = our_best_model_pred[:, :, 1]\n",
    "                our_inds = our_scores.topk(self.k, dim=0).indices\n",
    "                scores = model_pred[:, :, 1]\n",
    "                inds = scores.topk(self.k, dim=0).indices\n",
    "\n",
    "                if (sum(sorted(our_inds) - sorted(inds)) != 0:\n",
    "                    print(f\"Found diff at : {i}\")\n",
    "                    print(\"Our prediction: \", our_inds)\n",
    "                    print(\"Their prediction: \", inds)\n",
    "                    print(\"_________________________\")\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "eval_diff_predictions(\"/home/centos/bug_ml/new/training/l_logs_private/\", 'bert_caching')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
